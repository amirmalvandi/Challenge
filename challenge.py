# -*- coding: utf-8 -*-
"""Challenge

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hGQOZgOXs69mn9On-W_bSoGhLXTlRT-o
"""

import pandas as pd 
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv ('/content/drive/MyDrive/Colab Notebooks/Incidents_Responded_to_by_Fire_Companies.csv')
zip=pd.read_excel('/content/drive/MyDrive/Colab Notebooks/zip.xlsx')

df

df['INCIDENT_TYPE_DESC'].value_counts()/df.shape[0]

df['UNITS_ONSCENE'].value_counts()

df[df['INCIDENT_TYPE_DESC']=='111 - Building fire']['UNITS_ONSCENE'].mean()/df[df['INCIDENT_TYPE_DESC']=='651 - Smoke scare, odor of smoke']['UNITS_ONSCENE'].mean()

df[df['INCIDENT_TYPE_DESC']=='111 - Building fire']['UNITS_ONSCENE'].agg(['count','mean'])

df[df['INCIDENT_TYPE_DESC']=='651 - Smoke scare, odor of smoke']['UNITS_ONSCENE'].agg(['count','mean'])

bdf=df[df['INCIDENT_TYPE_DESC']=='710 - Malicious, mischievous false call, other'].groupby('BOROUGH_DESC').agg(['count','mean'])
bdf

8989/27551

df[df['INCIDENT_TYPE_DESC']=='111 - Building fire']['INCIDENT_DATE_TIME']

df[df['INCIDENT_TYPE_DESC']=='111 - Building fire']['ARRIVAL_DATE_TIME']

dtime =(pd.to_datetime(df[df['INCIDENT_TYPE_DESC']=='111 - Building fire']['ARRIVAL_DATE_TIME']).apply(lambda x: x.value)- pd.to_datetime(df[df['INCIDENT_TYPE_DESC']=='111 - Building fire']['INCIDENT_DATE_TIME']).apply(lambda x: x.value))/60/(10**9)

dtime.quantile([0.25,0.5,0.75])

df['ZIP_CODE'] = df['ZIP_CODE'].apply (pd.to_numeric, errors='coerce')
df.dropna()

gdf=df[df['INCIDENT_TYPE_DESC']=='111 - Building fire']
pdd=gdf['ZIP_CODE'].astype('int32')

zip

hdf=gdf.merge(zip, left_on='ZIP_CODE', right_on='Zip Code')

hdf[['ZIP_CODE','Zip Code','IM_INCIDENT_KEY','Population']]

zip_count=hdf[['Zip Code','Population']].groupby('Zip Code').count()

zip_mean=hdf[['Zip Code','Population']].groupby('Zip Code').mean()
zip_mean.columns = ['Residents']
zip_sort=hdf['Zip Code'].sort_values().unique()
zip_sort=pd.DataFrame(zip_sort)
zip_sort.columns = ['Zip_code']

merge_t=zip_sort.join(zip_mean, on='Zip_code', how='inner')
merge_t=merge_t.join(zip_count, on='Zip_code', how='inner')

merge_t

(merge_t['Population'].astype(float).corr(merge_t['Residents'].astype(float)))**2

merge_t.corr()**2

R=hdf[['ZIP_CODE','Zip Code','IM_INCIDENT_KEY','Population']].corr()
R**2

h_fire_incident=pd.DatetimeIndex(df['ARRIVAL_DATE_TIME'][df['INCIDENT_TYPE_DESC']=='113 - Cooking fire, confined to container']).hour

h_incident=pd.DatetimeIndex(df['ARRIVAL_DATE_TIME']).hour

total_df= h_incident.dropna().value_counts().sort_index()
fire_df=h_fire_incident.dropna().value_counts().sort_index()
fire_df

total_df

(fire_df/total_df)



idx=df['CO_DETECTOR_PRESENT_DESC'].dropna().index
df1=df.loc[idx, ['TOTAL_INCIDENT_DURATION', 'CO_DETECTOR_PRESENT_DESC']]

df1[ (df1['TOTAL_INCIDENT_DURATION']>60*60) & (df1['CO_DETECTOR_PRESENT_DESC']=='Yes')]

df2=df1[df1['CO_DETECTOR_PRESENT_DESC']=='No']

df2[df2['TOTAL_INCIDENT_DURATION']>60*60]

from scipy.stats import chi2_contingency
contigency= pd.crosstab(df2['TOTAL_INCIDENT_DURATION']<60*60, df2['TOTAL_INCIDENT_DURATION']>60*60) 
c, p, dof, expected = chi2_contingency(contigency) 
c



# Define parameters for the walk
dims = 3
step_n = 1000
step_set = [-1, 0, 1]
origin = np.zeros((1,dims))   # Simulate steps in 3D
step_shape = (step_n,dims)
steps = np.random.choice(a=step_set, size=step_shape)
path = np.concatenate([origin, steps]).cumsum(0)
start = path[:1]
stop = path[-1:]# Plot the path
fig = plt.figure(figsize=(10,10),dpi=200)
ax = fig.add_subplot(111, projection='3d')
ax.grid(False)
ax.xaxis.pane.fill = ax.yaxis.pane.fill = ax.zaxis.pane.fill = False
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
ax.scatter3D(path[:,0], path[:,1], path[:,2],c='blue', alpha=0.25,s=1)
ax.plot3D(path[:,0], path[:,1], path[:,2], c='blue', alpha=0.5, lw=0.5)
ax.plot3D(start[:,0], start[:,1], start[:,2], c='red', marker='+')
ax.plot3D(stop[:,0], stop[:,1], stop[:,2],  c='black', marker='o')
plt.title('3D Random Walk')
plt.savefig('random_walk_3d.png',dpi=50);

a=np.sqrt(3)/2
b=0.5
mat=np.matrix([[a, b], [0, 1], [-a, b], [0, -1], [-a, -b], [a, -b]])

mntc=100000
dist=[]
for j in range(mntc):
      arr=0
      for i in range(13):
          arr+=np.random.choice([1,0,0,0,0,0], size= (1,6), replace=False)
      location=np.matmul(arr,mat)
      dist.append (np.sqrt(location.item(1)**2+location.item(0)**2))

np.mean(dist)

np.std(dist)

gr_4 =np.sum([i for i in dist if i >= 4])
gr_4/mntc

gr_20 =np.sum([i for i in dist if i >= 20])
gr_15 =np.sum([i for i in dist if i >= 15])
gr_20/gr_15



import pandas as pd
from sklearn import preprocessing
import numpy as np
import matplotlib.pyplot as plt
import pandas.util.testing as tm
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from datetime import datetime
from scipy.linalg import toeplitz

dst = pd.read_csv('/content/drive/MyDrive/stock_data.csv')
dst['Date']= pd.to_datetime(dst['Date'])
dst['Year'] = dst['Date'].dt.year
dst['Month'] = dst['Date'].dt.month
dst['Day'] = dst['Date'].dt.day

dst

dst.Stock.unique()

s_apple = dst[dst['Stock']=='AAPL']

plt.plot(s_apple['Date'][100:500], s_apple['Open'][100:500], 'ro')

s_apple.shape

train, test = train_test_split(s_apple, test_size=0.3, random_state=33, shuffle=False)
P_train = train [['Year', 'Month', 'Day']]
R_train = train [ 'Close' ]
P_test = test [['Year', 'Month', 'Day']]
R_test = test [ 'Close' ]
ols_fit = sm.OLS(np.array(R_train), np.array(P_train)).fit()
ypred = ols_fit.predict(P_test)
fig, ax = plt.subplots()
ax.plot(R_test, ypred, "o", label="Data")
ax.legend(loc="best")







glsar_model = sm.GLSAR(np.array(R_train), np.array(P_train), 1)
glsar_results = glsar_model.iterative_fit(1)
ypred = glsar_results.predict(P_test)

glsar_results.summary()

fig, ax = plt.subplots()
ax.plot(R_test, ypred, "o", label="Data")
ax.legend(loc="best")

print(ols_fit.summary())

import requests
import csv
import pandas as pd
from sklearn import preprocessing
import numpy as np
import matplotlib.pyplot as plt
import pandas.util.testing as tm
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from datetime import datetime
from scipy.linalg import toeplitz

CSV_URL = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=AAPL&interval=15min&slice=year1month1&apikey=4JCILA50HKHQHG7I4'

with requests.Session() as s:
    download = s.get(CSV_URL)
    decoded_content = download.content.decode('utf-8')
    cr = csv.reader(decoded_content.splitlines(), delimiter=',')
    my_list = list(cr)

dfstock = pd.DataFrame(my_list)
dfstock.columns =dfstock.iloc[0]
dfstock=dfstock.drop(dfstock.index[0])
dfstock['time']= pd.to_datetime(dfstock['time'])
dfstock['Year'] = dfstock['time'].dt.year
dfstock['Month'] = dfstock['time'].dt.month
dfstock['Day'] = dfstock['time'].dt.day
dfstock['hour'] = dfstock['time'].dt.hour
dfstock['min'] = dfstock['time'].dt.minute
dfstock['second'] = dfstock['time'].dt.second

train, test = train_test_split(dfstock, test_size=0.3, random_state=33, shuffle=False)
P_train = train [['hour', 'min', 'open']].astype(float)
R_train = train ['close'].astype(float)
P_test = test [['hour', 'min', 'open']].astype(float)
R_test = test [ 'close' ].astype(float)

ols_fit = sm.OLS(np.array(R_train), np.array(P_train)).fit()
ypred = ols_fit.predict(P_test)
fig, ax = plt.subplots()
ax.plot(R_test, ypred, "o", label="Data")
ax.legend(loc="best")



